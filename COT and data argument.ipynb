{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8e29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3393bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy result: ['Hello', 'world', '.', 'This', 'is', 'a', 'test', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Hello world. This is a test sentence.\"\n",
    "\n",
    "# tokenize\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(\"spaCy result:\", tokens)\n",
    "\n",
    "import nltk\n",
    "#  nltk.download(\"wordnet\")\n",
    "#  nltk.download(\"omw-1.4\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69c074",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22753ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\conda\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, TrainingArguments,\n",
    "    Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import nlpaug.augmenter.word as naw\n",
    "import random\n",
    "import re\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig, PeftModel\n",
    "import gradio as gr\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "# Set constant\n",
    "MODEL_PATH = \"./gender-bias-detector\"\n",
    "MODEL_NAME = \"Qwen/Qwen1.5-1.8B\"\n",
    "TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d08d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unified quantitative configuration\n",
    "def get_bnb_config():\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        load_in_8bit_fp32_cpu_offload=True \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d2c88ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_fewshot_prompt(user_sentence: str) -> str:\n",
    "    cot_examples = [\n",
    "        (\n",
    "            \"Women belong in the kitchen.\",\n",
    "            \"Step 1: This sentence states 'Women belong in the kitchen.'\\n\"\n",
    "            \"Step 2: It enforces a gender stereotype suggesting women should stay in domestic roles.\\n\"\n",
    "            \"Step 3: This is a clear example of explicit bias.\\n\"\n",
    "            \"Answer: Explicit\"\n",
    "        ),\n",
    "        (\n",
    "            \"Women are more nurturing than men.\",\n",
    "            \"Step 1: The sentence claims 'Women are more nurturing than men.'\\n\"\n",
    "            \"Step 2: It implies a generalization about women’s behavior.\\n\"\n",
    "            \"Step 3: This is an implicit stereotype.\\n\"\n",
    "            \"Answer: Implicit\"\n",
    "        ),\n",
    "        (\n",
    "            \"He earned the job through hard work.\",\n",
    "            \"Step 1: The sentence says 'He earned the job through hard work.'\\n\"\n",
    "            \"Step 2: It contains no mention of gender or stereotype.\\n\"\n",
    "            \"Step 3: This is a neutral statement.\\n\"\n",
    "            \"Answer: Non\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # Splicing CoT example\n",
    "    fewshot = \"\\n\\n\".join(\n",
    "        f\"Sentence: {sent}\\nReasoning:\\n{reason}\"\n",
    "        for sent, reason in cot_examples\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        \"You are a gender bias classifier. For each sentence, follow the reasoning steps before predicting.\\n\"\n",
    "        \"Only output one line in the format: Answer: <label>, where <label> is one of:\\n\"\n",
    "        \"- Explicit\\n- Implicit\\n- Non\\n\\n\"\n",
    "        \"If none of these three labels apply, output Answer: Non.\\n\"\n",
    "        \"Do not output anything else.\\n\\n\"\n",
    "        f\"{fewshot}\\n\\n\"\n",
    "        f\"Sentence: {user_sentence.strip()}\\nReasoning:\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03640d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetAugmenter:\n",
    "    def __init__(self, spacy_model=\"en_core_web_sm\"):\n",
    "        self.nlp = spacy.load(spacy_model)\n",
    "\n",
    "    def get_synonyms(self, word, pos=None):\n",
    "        synonyms = set()\n",
    "        for syn in wordnet.synsets(word, pos=pos):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonym = lemma.name().replace(\"_\", \" \").lower()\n",
    "                if synonym != word.lower():\n",
    "                    synonyms.add(synonym)\n",
    "        return list(synonyms)\n",
    "\n",
    "    def spacy_pos_to_wordnet(self, spacy_pos):\n",
    "        if spacy_pos.startswith(\"N\"):\n",
    "            return wordnet.NOUN\n",
    "        elif spacy_pos.startswith(\"V\"):\n",
    "            return wordnet.VERB\n",
    "        elif spacy_pos.startswith(\"J\"):\n",
    "            return wordnet.ADJ\n",
    "        elif spacy_pos.startswith(\"R\"):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def augment(self, text, n_replace=1):\n",
    "        doc = self.nlp(text)\n",
    "        words = [token.text for token in doc]\n",
    "        candidates = []\n",
    "\n",
    "        for i, token in enumerate(doc):\n",
    "            if not token.is_alpha:\n",
    "                continue\n",
    "            wn_pos = self.spacy_pos_to_wordnet(token.tag_)\n",
    "            if wn_pos and wordnet.synsets(token.text, pos=wn_pos):\n",
    "                candidates.append((i, token.text, wn_pos))\n",
    "\n",
    "        if not candidates:\n",
    "            return text\n",
    "\n",
    "        random.shuffle(candidates)\n",
    "        replaced = 0\n",
    "        for idx, word, pos in candidates:\n",
    "            synonyms = self.get_synonyms(word, pos)\n",
    "            if synonyms:\n",
    "                words[idx] = random.choice(synonyms)\n",
    "                replaced += 1\n",
    "            if replaced >= n_replace:\n",
    "                break\n",
    "\n",
    "        return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "659cc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordnet_augmenter = None \n",
    "# Define enhancement function\n",
    "def augment_text(text, label, augment_rate=0.3):\n",
    "    global wordnet_augmenter\n",
    "    if label == \"implicit\" and random.random() < augment_rate and wordnet_augmenter:\n",
    "        return wordnet_augmenter.augment(text)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# %%\n",
    "def train_model(debug_mode=True):\n",
    "    \"\"\"Train the model and save it to\"\"\"\n",
    "    # load dataset\n",
    "    try:\n",
    "        df = pd.read_csv(\"dataset.csv\")\n",
    "        print(\"Loading a dataset from a CSV file\")\n",
    "        df[\"label\"] = df[\"label\"].str.lower().str.replace(\"-\", \"_\").str.strip()\n",
    "        df[\"label\"] = df[\"label\"].replace({\n",
    "            \"implicit_sexist\": \"implicit\",\n",
    "            \"explicit_sexist\": \"explicit\",\n",
    "            \"non_sexist\": \"non\",\n",
    "            \"non-sexist\": \"non\",\n",
    "            \"nonsexist\": \"non\",\n",
    "            \"sexist_implicit\": \"implicit\",\n",
    "            \"sexist_explicit\": \"explicit\"\n",
    "        })\n",
    "\n",
    "     #   if debug_mode:\n",
    "      #      print(\" Debug mode on: only 50 data points are sampled for each category\")\n",
    "     #       df = df.groupby(\"label\").apply(lambda x: x.sample(n=30, random_state=42)).reset_index(drop=True)\n",
    "        \n",
    "        global wordnet_augmenter\n",
    "        wordnet_augmenter = WordNetAugmenter()\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to load dataset: {e}\")\n",
    "        print(\"Create a sample dataset\")\n",
    "        data = {\n",
    "            'text': [\"She got the promotion because she's attractive.\"] * 1000 + \n",
    "                    [\"Women belong in the kitchen\"] * 1000 + \n",
    "                    [\"He earned the promotion through hard work\"] * 1000,\n",
    "            'label': [\"implicit\"] * 1000 + \n",
    "                     [\"explicit\"] * 1000 + \n",
    "                     [\"non\"] * 1000\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "    # Application enhancement: After group sampling, enhance the implicit class\n",
    "    df[\"text\"] = df.apply(lambda row: augment_text(row[\"text\"], row[\"label\"]), axis=1)\n",
    "\n",
    "    # Creating the Hugging Face dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # check dataset\n",
    "    print(f\"Data set size: {len(dataset)}\")\n",
    "    print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "    \n",
    "    # split dataset\n",
    "    split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "    \n",
    "    # load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        token=TOKEN\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  \n",
    "    \n",
    "    # data progress\n",
    "    def build_prompt(example):\n",
    "        prompt = build_fewshot_prompt(example[\"text\"])\n",
    "        # Update label mapping\n",
    "        label_mapping = {\n",
    "            \"implicit\": \"implicit\",\n",
    "            \"explicit\": \"explicit\",\n",
    "            \"non\": \"non\",\n",
    "        }\n",
    "        \n",
    "        # Get the label \n",
    "        target_label = str(example[\"label\"]).lower().strip()\n",
    "        full_text = prompt + f\"Answer: {target_label.capitalize()}{tokenizer.eos_token}\"\n",
    "\n",
    "        tokenized = tokenizer(full_text, truncation=True, max_length=900, padding=\"max_length\")\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "\n",
    "        prompt_len = len(tokenizer(prompt, add_special_tokens=False)[\"input_ids\"])\n",
    "        labels = [-100] * prompt_len + input_ids[prompt_len:]\n",
    "        labels = labels[:900]\n",
    "        if len(labels) < 900:\n",
    "            labels += [-100] * (900 - len(labels))\n",
    "\n",
    "        tokenized[\"labels\"] = labels\n",
    "        return tokenized\n",
    "\n",
    "    # Application prompt project\n",
    "    tokenized_train = train_dataset.map(build_prompt, remove_columns=['text', 'label'])\n",
    "    tokenized_eval = eval_dataset.map(build_prompt, remove_columns=['text', 'label'])\n",
    "    \n",
    "    # Use a unified quantization configuration\n",
    "    bnb_config = get_bnb_config()\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        token=TOKEN\n",
    "    )\n",
    "\n",
    "    # LoRA configuration\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # training configuration\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=MODEL_PATH,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_steps=20,\n",
    "        num_train_epochs=3,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-4,\n",
    "        bf16=False,\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=2\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    # train model\n",
    "    print(\"start train model...\")\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def extract_answer(txt):\n",
    "        import re\n",
    "        m = re.findall(r\"answer:\\s*(explicit|implicit|non)\", txt, re.I)\n",
    "        return (m[-1] if m else \"non\").lower()\n",
    "\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    for example in eval_dataset:\n",
    "        prompt = build_fewshot_prompt(example[\"text\"])\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=900).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=16,\n",
    "                temperature=0.0,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                num_return_sequences=1,\n",
    "                return_dict_in_generate=False\n",
    "            )\n",
    "\n",
    "            \n",
    "  \n",
    "        \n",
    "\n",
    "        # Clean up punctuation and trailing spaces\n",
    "        \n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n",
    "        label_candidate = extract_answer(decoded)\n",
    "        \n",
    "        predictions.append(label_candidate)\n",
    "        true_labels.append(example[\"label\"])\n",
    "\n",
    "    #    print(f\" model result: {decoded}\")\n",
    "    \n",
    "\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Verify assessment results:\")\n",
    "    for k, v in eval_results.items():\n",
    "        if isinstance(v, (float, int)):\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "\n",
    "    # save model\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "    print(f\"Training complete! Model saved to {MODEL_PATH}\")\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    print(\"Evaluate model performance...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Validation loss: {eval_results['eval_loss']}\")\n",
    "\n",
    "     # Clean memory\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44a8f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Uncomment the following line to run the training\n",
    "#train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13893326",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9029fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load base model...\n",
      "load adapter...\n",
      "Number of trainable parameters:\n",
      "trainable params: 0 || all params: 1,843,120,128 || trainable%: 0.0000\n",
      "Current Peft configuration： {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='Qwen/Qwen1.5-1.8B', revision=None, inference_mode=True, r=16, target_modules={'q_proj', 'o_proj', 'v_proj', 'k_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
      "activated LoRA: ['default']\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "MODEL_PATH = \"./gender-bias-detector\"\n",
    "MODEL_NAME = \"Qwen/Qwen1.5-1.8B\"\n",
    "TOKEN = \"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\"\n",
    "# %%\n",
    "def load_model():\n",
    "    \"\"\"Loading a trained model from disk\"\"\"\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model catalog {MODEL_PATH} does not exist. Please run the training part first.\")\n",
    "\n",
    "    print(\"Load base model...\")\n",
    "    bnb_config = get_bnb_config()\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map={\"\": 0},  \n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        token=TOKEN\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"load adapter...\")\n",
    "    model = PeftModel.from_pretrained(base_model, MODEL_PATH,is_trainable=False).eval()\n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    print(\"Number of trainable parameters:\")\n",
    "    model.print_trainable_parameters()\n",
    "    print(\"Current Peft configuration：\", model.peft_config)\n",
    "    print(\"activated LoRA:\", model.active_adapters)\n",
    "    return model, tokenizer\n",
    "\n",
    "# %%\n",
    "# load model\n",
    "try:\n",
    "    model, tokenizer = load_model()\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    print(\"Please run the training part first or make sure the model is saved correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dcca5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually prepare eval_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "\n",
    "#df = df.groupby(\"label\").apply(lambda x: x.sample(n=100, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "df[\"label\"] = df[\"label\"].str.lower().str.replace(\"-\", \"_\").str.strip()\n",
    "df[\"label\"] = df[\"label\"].replace({\n",
    "    \"implicit_sexist\": \"implicit\",\n",
    "    \"explicit_sexist\": \"explicit\",\n",
    "    \"non_sexist\": \"non\",\n",
    "    \"non-sexist\": \"non\",\n",
    "    \"nonsexist\": \"non\",\n",
    "    \"sexist_implicit\": \"implicit\",\n",
    "    \"sexist_explicit\": \"explicit\"\n",
    "})\n",
    "\n",
    "# Apply enhancement logic\n",
    "df[\"text\"] = df.apply(lambda row: augment_text(row[\"text\"], row[\"label\"]), axis=1)\n",
    "\n",
    "# split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c258e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 分类报告（Explicit / Implicit / Non）\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    explicit     0.8904    0.9112    0.9007       214\n",
      "    implicit     0.9126    0.8350    0.8721       200\n",
      "         non     0.8779    0.9303    0.9034       201\n",
      "\n",
      "    accuracy                         0.8927       615\n",
      "   macro avg     0.8936    0.8922    0.8920       615\n",
      "weighted avg     0.8935    0.8927    0.8923       615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "VALID = {\"explicit\", \"implicit\", \"non\"}\n",
    "\n",
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"Grab the last Answer: tag from the generated text, and use it as a backup.\"\"\"\n",
    "    m = re.findall(r\"answer:\\s*(explicit|implicit|non)\", text, re.I)\n",
    "    return m[-1].lower() if m else \"non\"\n",
    "\n",
    "def evaluate_model_on_test_set(model, tokenizer, eval_dataset):\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "\n",
    "    for ex in eval_dataset:\n",
    "        prompt = build_fewshot_prompt(ex[\"text\"])          #  same to train parse\n",
    "        ids = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=900,\n",
    "            padding=False         \n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **ids,\n",
    "                max_new_tokens=16,          \n",
    "                do_sample=False,           \n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "                \n",
    "            )\n",
    "\n",
    "   \n",
    "        decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        pred = extract_answer(decoded)\n",
    "\n",
    "\n",
    "\n",
    "        preds.append(pred)\n",
    "        truths.append(ex[\"label\"].lower())\n",
    "\n",
    "    print(\"\\n classification_report（Explicit / Implicit / Non）\")\n",
    "    print(classification_report(\n",
    "        truths, preds,\n",
    "        labels=[\"explicit\", \"implicit\", \"non\"],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "evaluate_model_on_test_set(model, tokenizer, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1eff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3bf6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "def classify_gender_bias(text):\n",
    "    \"\"\"Using the model for gender-biased classification)\"\"\"\n",
    "    prompt = build_fewshot_prompt(text)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=900).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=16,\n",
    "            num_beams=1,\n",
    "            early_stopping=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.0,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract lines starting with \"Answer:\"\n",
    "    answer_lines = [line for line in decoded.splitlines() if line.lower().startswith(\"answer:\")]\n",
    "    if answer_lines:\n",
    "        content = answer_lines[-1][len(\"answer:\"):].strip().lower()\n",
    "\n",
    "        # Match in order to avoid early false hits of non\n",
    "        if re.fullmatch(r\"explicit\", content, re.IGNORECASE):\n",
    "            pred = \"explicit\"\n",
    "        elif re.fullmatch(r\"implicit\", content, re.IGNORECASE):\n",
    "            pred = \"implicit\"\n",
    "        elif re.fullmatch(r\"non\", content, re.IGNORECASE):\n",
    "            pred = \"non\"\n",
    "        else:\n",
    "            print(\" Unmatched tag content：\", content)\n",
    "            pred = \"unknown\"\n",
    "    else:\n",
    "        print(\"Unable to find the Answer line, original output：\", decoded)\n",
    "        pred = \"unknown\"\n",
    "\n",
    "    return pred.capitalize() if pred != \"unknown\" else \"Unknown\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87011f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee4da85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'She only got the job because she's pretty'\n",
      "result: Implicit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'Women are really bad drivers.'\n",
      "result: Explicit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'He earned the promotion through hard work'\n",
      "result: Non\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'Men are better leaders than women'\n",
      "result: Explicit\n",
      "\n",
      "text: 'Act like a lady , think like a man ? ..'\n",
      "result: Implicit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Test the classification function\n",
    "if 'model' in locals() and 'tokenizer' in locals():\n",
    "    test_texts = [\n",
    "        \"She only got the job because she's pretty\",\n",
    "        \"Women are really bad drivers.\",\n",
    "        \"He earned the promotion through hard work\",\n",
    "        \"Men are better leaders than women\",\n",
    "        \"Act like a lady , think like a man ? ..\"\n",
    "    ]\n",
    "\n",
    "    for text in test_texts:\n",
    "        result = classify_gender_bias(text)\n",
    "        print(f\"text: '{text}'\")\n",
    "        print(f\"result: {result}\\n\")\n",
    "        \n",
    "else:\n",
    "    print(\"Model not loaded, cannot be tested\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b100e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94ef1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Creating the Gradio Interactive Interface\"\"\"\n",
    "    # Define a list of sample text\n",
    "    example_texts = [\n",
    "        \"She only got the job because she's pretty\",\n",
    "        \"Women are really bad drivers.\",\n",
    "        \"Real women don't go along w/ crap! It's a fake cause manufactured by corporations.\",\n",
    "        \"Men are better leaders than women\",\n",
    "        \"The nurse took care of the patient while the doctor performed surgery\",\n",
    "        \"Act like a lady , think like a man ? ..\"\n",
    "    ]\n",
    "    \n",
    "    # Create interface\n",
    "    with gr.Blocks(title=\"Gender Bias Detector\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"#  Gender Bias Detection\")\n",
    "        gr.Markdown(\"Identify implicit or explicit gender bias in text\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            input_text = gr.Textbox(\n",
    "                label=\"Enter text to analyze\",\n",
    "                placeholder=\"Type a sentence here...\",\n",
    "                lines=3\n",
    "            )\n",
    "            \n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Analyze\", variant=\"primary\")\n",
    "            \n",
    "        with gr.Row():\n",
    "            output_label = gr.Label(label=\"Classification Result\")\n",
    "            \n",
    "        with gr.Row():\n",
    "            gr.Examples(\n",
    "                examples=example_texts, \n",
    "                inputs=[input_text],  \n",
    "                label=\"Example Sentences\"\n",
    "            )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=classify_gender_bias,\n",
    "            inputs=input_text,\n",
    "            outputs=output_label\n",
    "        )\n",
    "    \n",
    "    return demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157ff380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "# Create and display the interface\n",
    "if 'model' in locals() and 'tokenizer' in locals():\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(share=False, inline=True) \n",
    "else:\n",
    "    print(\"The model is not loaded and the interface cannot be created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5ca26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
