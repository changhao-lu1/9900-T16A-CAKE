{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8e29d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69c074",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22753ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\conda\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    BitsAndBytesConfig, TrainingArguments,\n",
    "    Trainer, DataCollatorForLanguageModeling\n",
    ")\n",
    "import random\n",
    "import re\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig, PeftModel\n",
    "import gradio as gr\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "# Set constant\n",
    "MODEL_PATH = \"./gender-bias-detector\"\n",
    "MODEL_NAME = \"Qwen/Qwen1.5-1.8B\"\n",
    "TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5585316",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659cc779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unified quantization configuration \n",
    "def get_bnb_config():\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        load_in_8bit_fp32_cpu_offload=True \n",
    "    )\n",
    "\n",
    "\n",
    "def build_fewshot_prompt(user_sentence: str) -> str:\n",
    "    examples = [\n",
    "        (\"Women belong in the kitchen.\", \"Explicit\"),\n",
    "        (\"Women are more nurturing than men.\", \"Implicit\"),\n",
    "        (\"He earned the job through hard work.\", \"Non\"),\n",
    "    ]\n",
    "    random.shuffle(examples)\n",
    "    fewshot = \"\\n\\n\".join([f\"Sentence: {ex[0]}\\nAnswer: {ex[1]}\" for ex in examples])\n",
    "    return (\n",
    "        \"You are a text classifier. Only answer with: Implicit, Explicit, or Non.\\n\"\n",
    "        \"- Implicit\\n- Explicit\\n- Non\\n\"\n",
    "        \"Respond only with one category.\\n\\n\"\n",
    "        f\"{fewshot}\\n\\n\"\n",
    "        f\"Sentence: {user_sentence}\\nAnswer:\"\n",
    "    )\n",
    "# %%\n",
    "def train_model(debug_mode=True):\n",
    "    \"\"\"Train the model and save it to disk\"\"\"\n",
    "    # Load dataset\n",
    "    try:\n",
    "        df = pd.read_csv(\"dataset.csv\")\n",
    "        print(\"Loading a dataset from a CSV file\")\n",
    "        df[\"label\"] = df[\"label\"].str.lower().str.replace(\"-\", \"_\").str.strip()\n",
    "        df[\"label\"] = df[\"label\"].replace({\n",
    "            \"implicit_sexist\": \"implicit\",\n",
    "            \"explicit_sexist\": \"explicit\",\n",
    "            \"non_sexist\": \"non\",\n",
    "            \"non-sexist\": \"non\",\n",
    "            \"nonsexist\": \"non\",\n",
    "            \"sexist_implicit\": \"implicit\",\n",
    "            \"sexist_explicit\": \"explicit\"\n",
    "        })\n",
    "\n",
    "        if debug_mode:\n",
    "            print(\" Debug mode on: only 50 data points are sampled for each category\")\n",
    "            df = df.groupby(\"label\").apply(lambda x: x.sample(n=1000, random_state=42)).reset_index(drop=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unable to load dataset: {e}\")\n",
    "        print(\"Create a sample dataset\")\n",
    "        data = {\n",
    "            'text': [\"She got the promotion because she's attractive.\"] * 1000 + \n",
    "                    [\"Women belong in the kitchen\"] * 1000 + \n",
    "                    [\"He earned the promotion through hard work\"] * 1000,\n",
    "            'label': [\"implicit\"] * 1000 + \n",
    "                     [\"explicit\"] * 1000 + \n",
    "                     [\"non\"] * 1000\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "\n",
    "\n",
    "    # Creating the Hugging Face dataset\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # Check the data set\n",
    "    print(f\"Data set size: {len(dataset)}\")\n",
    "    print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "    \n",
    "    # Split the dataset\n",
    "    split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "    \n",
    "    # Loading the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        token=TOKEN\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # data progress\n",
    "    def build_prompt(example):\n",
    "        prompt = build_fewshot_prompt(example[\"text\"])\n",
    "        # Update label mapping\n",
    "        label_mapping = {\n",
    "            \"implicit\": \"implicit\",\n",
    "            \"explicit\": \"explicit\",\n",
    "            \"non\": \"non\",\n",
    "        }\n",
    "        \n",
    "        # Get the label\n",
    "\n",
    "        label = str(example[\"label\"]).lower().strip()\n",
    "        \n",
    "        # Find matching tags\n",
    "        target_label = \"non\"  \n",
    "        for key, value in label_mapping.items():\n",
    "            if key in label:\n",
    "                target_label = value\n",
    "                break\n",
    "        \n",
    "        \n",
    "        full_text = prompt + target_label\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            padding=\"max_length\",\n",
    "            \n",
    "        )\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "\n",
    "        # Get the prompt length\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "        prompt_len = len(prompt_ids)\n",
    "\n",
    "        # Construct labels\n",
    "        labels = [-100] * prompt_len + input_ids[prompt_len:]\n",
    "        labels = labels[:1024]  \n",
    "        if len(labels) < 1024:\n",
    "            labels += [-100] * (1024 - len(labels))  \n",
    "        tokenized[\"labels\"] = labels\n",
    "        return tokenized\n",
    "\n",
    "    # Application prompt project\n",
    "    tokenized_train = train_dataset.map(build_prompt, remove_columns=['text', 'label'])\n",
    "    tokenized_eval = eval_dataset.map(build_prompt, remove_columns=['text', 'label'])\n",
    "    \n",
    "    # Use a unified quantization configuration\n",
    "    bnb_config = get_bnb_config()\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        token=TOKEN\n",
    "    )\n",
    "\n",
    "    # LoRA configuration\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    \n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # training configuration\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=MODEL_PATH,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_steps=20,\n",
    "        num_train_epochs=3,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-4,\n",
    "        bf16=False,\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=2\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "    \n",
    "    # train model\n",
    "    print(\"start training model...\")\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "    # save model\n",
    "    model.save_pretrained(MODEL_PATH)\n",
    "    tokenizer.save_pretrained(MODEL_PATH)\n",
    "    print(f\"Training complete! Model saved to {MODEL_PATH}\")\n",
    "    \n",
    "    # Evaluate model performance\n",
    "    print(\"Evaluate model performance...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Validation loss: {eval_results['eval_loss']}\")\n",
    "\n",
    "     # Clean memory\n",
    "    del model\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01607b51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a8f344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a dataset from a CSV file\n",
      " Debug mode on: only 50 data points are sampled for each category\n",
      "Data set size: 3000\n",
      "Label distribution:\n",
      "label\n",
      "explicit    1000\n",
      "implicit    1000\n",
      "non         1000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\卢常昊\\AppData\\Local\\Temp\\ipykernel_53460\\130363632.py:47: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(\"label\").apply(lambda x: x.sample(n=1000, random_state=42)).reset_index(drop=True)\n",
      "Map: 100%|██████████| 2400/2400 [00:02<00:00, 1042.63 examples/s]\n",
      "Map: 100%|██████████| 600/600 [00:00<00:00, 1071.00 examples/s]\n",
      "C:\\Users\\卢常昊\\AppData\\Local\\Temp\\ipykernel_53460\\130363632.py:186: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,291,456 || all params: 1,843,120,128 || trainable%: 0.3413\n",
      "start training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "c:\\conda\\envs\\llm\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 3:46:45, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.441800</td>\n",
       "      <td>1.193988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.061000</td>\n",
       "      <td>1.158963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.969100</td>\n",
       "      <td>1.150918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\conda\\envs\\llm\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\conda\\envs\\llm\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete! Model saved to ./gender-bias-detector\n",
      "Evaluate model performance...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 06:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.150917887687683\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Uncomment the following line to run the training\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13893326",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9029fc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load base model...\n",
      "load adapter...\n",
      "Number of trainable parameters:\n",
      "trainable params: 0 || all params: 1,843,120,128 || trainable%: 0.0000\n",
      "Current Peft configuration： {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='Qwen/Qwen1.5-1.8B', revision=None, inference_mode=True, r=16, target_modules={'k_proj', 'v_proj', 'q_proj', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
      "activated LoRA: ['default']\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "MODEL_PATH = \"./gender-bias-detector\"\n",
    "MODEL_NAME = \"Qwen/Qwen1.5-1.8B\"\n",
    "TOKEN = \"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\"\n",
    "# %%\n",
    "def load_model():\n",
    "    \"\"\"Loading a trained model from disk\"\"\"\n",
    "\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        raise FileNotFoundError(f\"Model catalog {MODEL_PATH} does not exist. Please run the training part first.\")\n",
    "\n",
    "    print(\"Load base model...\")\n",
    "    bnb_config = get_bnb_config()\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map={\"\": 0},  \n",
    "        trust_remote_code=True,\n",
    "        quantization_config=bnb_config,\n",
    "        token=TOKEN\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"load adapter...\")\n",
    "    model = PeftModel.from_pretrained(base_model, MODEL_PATH,is_trainable=False).eval()\n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    tokenizer.pad_token = tokenizer.eos_token \n",
    "    print(\"Number of trainable parameters:\")\n",
    "    model.print_trainable_parameters()\n",
    "    print(\"Current Peft configuration：\", model.peft_config)\n",
    "    print(\"activated LoRA:\", model.active_adapters)\n",
    "    return model, tokenizer\n",
    "\n",
    "# %%\n",
    "# load model\n",
    "try:\n",
    "    model, tokenizer = load_model()\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "    print(\"Please run the training part first or make sure the model is saved correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfec5c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually prepare eval_dataset\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "\n",
    "\n",
    "#df = df.groupby(\"label\").apply(lambda x: x.sample(n=100, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "df[\"label\"] = df[\"label\"].str.lower().str.replace(\"-\", \"_\").str.strip()\n",
    "df[\"label\"] = df[\"label\"].replace({\n",
    "    \"implicit_sexist\": \"implicit\",\n",
    "    \"explicit_sexist\": \"explicit\",\n",
    "    \"non_sexist\": \"non\",\n",
    "    \"non-sexist\": \"non\",\n",
    "    \"nonsexist\": \"non\",\n",
    "    \"sexist_implicit\": \"implicit\",\n",
    "    \"sexist_explicit\": \"explicit\"\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7498b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " classification_report（Explicit / Implicit / Non）\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    explicit     0.8632    0.8551    0.8592       214\n",
      "    implicit     0.8390    0.8600    0.8494       200\n",
      "         non     0.9343    0.9204    0.9273       201\n",
      "\n",
      "    accuracy                         0.8780       615\n",
      "   macro avg     0.8789    0.8785    0.8786       615\n",
      "weighted avg     0.8786    0.8780    0.8783       615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "VALID = {\"explicit\", \"implicit\", \"non\"}\n",
    "\n",
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"Grab the last Answer: tag from the generated text, and use it as a backup.\"\"\"\n",
    "    m = re.findall(r\"answer:\\s*(explicit|implicit|non)\", text, re.I)\n",
    "    return m[-1].lower() if m else \"non\"\n",
    "\n",
    "def evaluate_model_on_test_set(model, tokenizer, eval_dataset):\n",
    "    model.eval()\n",
    "    preds, truths = [], []\n",
    "\n",
    "    for ex in eval_dataset:\n",
    "        prompt = build_fewshot_prompt(ex[\"text\"])          \n",
    "        ids = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=900,\n",
    "            padding=False         \n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **ids,\n",
    "                max_new_tokens=16,          \n",
    "                do_sample=False,           \n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "                \n",
    "            )\n",
    "\n",
    "   \n",
    "        decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        pred = extract_answer(decoded)\n",
    "\n",
    "\n",
    "\n",
    "        preds.append(pred)\n",
    "        truths.append(ex[\"label\"].lower())\n",
    "\n",
    "    print(\"\\n classification_report（Explicit / Implicit / Non）\")\n",
    "    print(classification_report(\n",
    "        truths, preds,\n",
    "        labels=[\"explicit\", \"implicit\", \"non\"],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "evaluate_model_on_test_set(model, tokenizer, eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1eff7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3bf6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_gender_bias(text):\n",
    "    \"\"\"Using the model for gender-biased classification)\"\"\"\n",
    "    prompt = build_fewshot_prompt(text)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=900).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=16,\n",
    "            num_beams=1,\n",
    "            early_stopping=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            temperature=0.0,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "    # Extract lines starting with \"Answer:\"\n",
    "    answer_lines = [line for line in decoded.splitlines() if line.lower().startswith(\"answer:\")]\n",
    "    if answer_lines:\n",
    "        content = answer_lines[-1][len(\"answer:\"):].strip().lower()\n",
    "\n",
    "        # Match in order to avoid early false hits of non\n",
    "        if re.fullmatch(r\"explicit\", content, re.IGNORECASE):\n",
    "            pred = \"explicit\"\n",
    "        elif re.fullmatch(r\"implicit\", content, re.IGNORECASE):\n",
    "            pred = \"implicit\"\n",
    "        elif re.fullmatch(r\"non\", content, re.IGNORECASE):\n",
    "            pred = \"non\"\n",
    "        else:\n",
    "            print(\" Unmatched tag content：\", content)\n",
    "            pred = \"non\"\n",
    "    else:\n",
    "        print(\"Unable to find the Answer line, original output：\", decoded)\n",
    "        pred = \"non\"\n",
    "\n",
    "    return pred.capitalize() if pred != \"unknown\" else \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce87011f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee4da85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'She only got the job because she's pretty'\n",
      "result: Implicit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'Women are really bad drivers.'\n",
      "result: Explicit\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'He earned the promotion through hard work'\n",
      "result: Non\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 'On the job, men should not refuse to work under women'\n",
      "result: Non\n",
      "\n",
      "text: 'Act like a lady , think like a man ? '\n",
      "result: Implicit\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Test classification function\n",
    "if 'model' in locals() and 'tokenizer' in locals():\n",
    "    test_texts = [\n",
    "        \"She only got the job because she's pretty\",\n",
    "        \"Women are really bad drivers.\",\n",
    "        \"He earned the promotion through hard work\",\n",
    "        \"On the job, men should not refuse to work under women\",\n",
    "        \"Act like a lady , think like a man ? \"\n",
    "    ]\n",
    "\n",
    "    for text in test_texts:\n",
    "        result = classify_gender_bias(text)\n",
    "        print(f\"text: '{text}'\")\n",
    "        print(f\"result: {result}\\n\")\n",
    "        \n",
    "else:\n",
    "    print(\"Model not loaded, cannot be tested\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b100e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94ef1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_gradio_interface():\n",
    "    \"\"\"Creating the Gradio Interactive Interface\"\"\"\n",
    "    # Define a list of sample text\n",
    "    example_texts = [\n",
    "        \"She only got the job because she's pretty\",\n",
    "        \"Women are really bad drivers.\",\n",
    "        \"Real women don't go along w/ crap! It's a fake cause manufactured by corporations.\",\n",
    "        \"Men are better leaders than women\",\n",
    "        \"The nurse took care of the patient while the doctor performed surgery\",\n",
    "        \"Act like a lady , think like a man ? ..\"\n",
    "    ]\n",
    "    \n",
    "    # Create interface\n",
    "    with gr.Blocks(title=\"Gender Bias Detector\", theme=gr.themes.Soft()) as demo:\n",
    "        gr.Markdown(\"#  Gender Bias Detection\")\n",
    "        gr.Markdown(\"Identify implicit or explicit gender bias in text\")\n",
    "        \n",
    "        with gr.Row():\n",
    "            input_text = gr.Textbox(\n",
    "                label=\"Enter text to analyze\",\n",
    "                placeholder=\"Type a sentence here...\",\n",
    "                lines=3\n",
    "            )\n",
    "            \n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Analyze\", variant=\"primary\")\n",
    "            \n",
    "        with gr.Row():\n",
    "            output_label = gr.Label(label=\"Classification Result\")\n",
    "            \n",
    "        with gr.Row():\n",
    "            gr.Examples(\n",
    "                examples=example_texts, \n",
    "                inputs=[input_text],  \n",
    "                label=\"Example Sentences\"\n",
    "            )\n",
    "        \n",
    "        submit_btn.click(\n",
    "            fn=classify_gender_bias,\n",
    "            inputs=input_text,\n",
    "            outputs=output_label\n",
    "        )\n",
    "    \n",
    "    return demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "157ff380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "#  Create and display the interface\n",
    "if 'model' in locals() and 'tokenizer' in locals():\n",
    "    demo = create_gradio_interface()\n",
    "    demo.launch(share=False, inline=True)  \n",
    "else:\n",
    "    print(\"模型未加载，无法创建界面\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e5ca26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
